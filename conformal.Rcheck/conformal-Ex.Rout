
R version 3.0.1 (2013-05-16) -- "Good Sport"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin10.8.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "conformal"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('conformal')
Loading required package: caret
Warning: package 'caret' was built under R version 3.0.2
Loading required package: lattice
Warning: package 'lattice' was built under R version 3.0.2
Loading required package: ggplot2
Loading required package: grid
Conformal prediction in R. Isidro Cortes-Ciriano <isidrolauscher@gmail.com>
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("ConformalClassification")
> ### * ConformalClassification
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ConformalClassification
> ### Title: Conformal Prediction For Classification
> ### Aliases: ConformalClassification
> 
> ### ** Examples
> 
> # Optional for parallel training
> #library(doMC)
> #registerDoMC(cores=4)
> 
> data(LogS)
> 
> # convert data to categorical
> LogSTrain[LogSTrain > -4] <- 1
> LogSTrain[LogSTrain <= -4] <- 2
> LogSTest[LogSTest > -4] <- 1
> LogSTest[LogSTest <= -4] <- 2
> 
> LogSTrain <- factor(LogSTrain)
> LogSTest <- factor(LogSTest)
> 
> algorithm <- "rf"
> 
> trControl <- trainControl(method = "cv",  number=5,savePredictions=TRUE)#,  predict.all=TRUE,keep.forest=TRUE,norm.votes=TRUE)
> set.seed(3)
> model <- train(LogSDescsTrain, LogSTrain, algorithm,type="classification", trControl=trControl,predict.all=TRUE,keep.forest=TRUE,norm.votes=TRUE)
Loading required package: randomForest
randomForest 4.6-7
Type rfNews() to see new features/changes/bug fixes.
Warning: package 'e1071' was built under R version 3.0.2
> 
> 
> # Instantiate the class and get the p.values
> example <- ConformalClassification$new()
Conformal Prediction Class for Classification Instantiated
> example$CalculateCVAlphas(model=model)
[1] "Calculating the vector of nonconformity measures for the CV predictions (label wise Mondrian ICP).."

> example$CalculatePValues(new.data=LogSDescsTest)
[1] "Classifying the input data.."

> example$p.values$P.values
                 1         2
427.1   0.43501946 0.5649805
598.1   0.60700389 0.0000000
919.1   0.53852140 0.4614786
1456.1  0.39922179 0.6007782
324.1   0.60700389 0.3929961
1439.1  0.40311284 0.5968872
1512.1  0.60700389 0.0000000
1057.1  0.60700389 0.3929961
1006.1  0.60700389 0.0000000
99.1    0.52918288 0.4708171
329.1   0.55719844 0.4420233
282.1   0.60700389 0.0000000
1096.1  0.60700389 0.3929961
612.1   0.60700389 0.0000000
1226.1  0.46225681 0.5377432
792.1   0.01400778 0.9859922
1142.1  0.12529183 0.8747082
1577.1  0.05291829 0.9470817
604.1   0.60700389 0.0000000
1234.1  0.38521401 0.6147860
1483.1  0.60700389 0.3929961
337.1   0.60700389 0.0000000
1033.1  0.03579767 0.9642023
199.1   0.18832685 0.8116732
423.1   0.60700389 0.0000000
611.1   0.53929961 0.4607004
22.1    0.60700389 0.0000000
1588.1  0.60700389 0.0000000
1373.1  0.60700389 0.0000000
537.1   0.29494163 0.7050584
760.1   0.31828794 0.6817121
945.1   0.60700389 0.0000000
777.1   0.20700389 0.7929961
293.1   0.56186770 0.4381323
1301.1  0.60700389 0.0000000
1051.1  0.44747082 0.5501946
1247.1  0.55719844 0.4420233
170.1   0.60700389 0.0000000
1135.1  0.37276265 0.6272374
645.1   0.36731518 0.6326848
1286.1  0.60700389 0.0000000
1013.1  0.60700389 0.0000000
1225.1  0.34241245 0.6575875
865.1   0.56031128 0.4396887
828.1   0.60700389 0.3929961
1233.1  0.20233463 0.7976654
37.1    0.60700389 0.0000000
745.1   0.53852140 0.4614786
1141.1  0.12529183 0.8747082
1079.1  0.25914397 0.7408560
744.1   0.60700389 0.3929961
1340.1  0.06926070 0.9307393
681.1   0.46225681 0.5377432
381.1   0.13774319 0.8622568
110.1   0.34163424 0.6583658
155.1   0.49805447 0.5019455
491.1   0.13073930 0.8692607
804.1   0.11128405 0.8887160
1025.1  0.31517510 0.6848249
630.1   0.37042802 0.6295720
1412.1  0.60700389 0.0000000
454.1   0.22957198 0.7704280
709.1   0.60700389 0.0000000
513.1   0.41556420 0.5844358
1004.1  0.60700389 0.0000000
398.1   0.24824903 0.7517510
737.1   0.47704280 0.5229572
1180.1  0.05836576 0.9416342
130.1   0.27470817 0.7252918
1346.1  0.30817121 0.6918288
521.1   0.60700389 0.0000000
1289.1  0.60700389 0.0000000
532.1   0.32529183 0.6747082
512.1   0.60700389 0.0000000
730.1   0.56186770 0.4381323
1366.1  0.53852140 0.4614786
1323.1  0.37898833 0.6210117
597.1   0.60700389 0.0000000
1188.NA 0.00000000 0.9859922
1467.1  0.60700389 0.0000000
664.1   0.60700389 0.0000000
1087.1  0.49805447 0.5019455
610.1   0.60700389 0.0000000
496.1   0.60700389 0.0000000
1153.1  0.37821012 0.6217899
309.1   0.53852140 0.4614786
1081.1  0.60700389 0.0000000
185.1   0.30817121 0.6918288
373.1   0.60700389 0.0000000
218.1   0.11750973 0.8824903
364.1   0.60700389 0.0000000
90.1    0.60700389 0.0000000
973.1   0.21634241 0.7836576
1326.1  0.56186770 0.4381323
1178.1  0.51050584 0.4894942
1205.1  0.39377432 0.6062257
688.1   0.24435798 0.7556420
619.1   0.40311284 0.5968872
1223.1  0.47159533 0.5284047
912.1   0.50428016 0.4957198
987.1   0.01400778 0.9859922
1534.1  0.46225681 0.5377432
407.1   0.60700389 0.3929961
1493.1  0.60700389 0.0000000
952.1   0.09027237 0.9097276
321.1   0.60700389 0.3929961
195.1   0.14085603 0.8591440
717.1   0.39377432 0.6062257
1385.1  0.34552529 0.6544747
897.1   0.60700389 0.0000000
1461.1  0.27159533 0.7284047
1095.1  0.60700389 0.3929961
533.1   0.42256809 0.5774319
1567.1  0.60700389 0.0000000
222.1   0.60700389 0.3929961
20.1    0.60700389 0.3929961
1067.1  0.60700389 0.3929961
154.1   0.43190661 0.5680934
665.1   0.60700389 0.0000000
1502.1  0.60700389 0.0000000
1474.1  0.02645914 0.9735409
736.1   0.49027237 0.5097276
719.1   0.29182879 0.7081712
258.1   0.13696498 0.8630350
1119.1  0.12140078 0.8785992
673.1   0.60700389 0.3929961
757.1   0.60700389 0.3929961
307.1   0.60700389 0.0000000
338.1   0.60700389 0.0000000
880.1   0.03891051 0.9610895
849.1   0.10583658 0.8941634
114.1   0.32529183 0.6747082
53.1    0.18910506 0.8108949
947.1   0.20856031 0.7914397
1367.1  0.60700389 0.0000000
1477.1  0.42490272 0.5750973
825.1   0.60700389 0.0000000
773.1   0.16653696 0.8334630
1447.1  0.20000000 0.8000000
1559.1  0.28560311 0.7143969
1001.1  0.60700389 0.0000000
882.1   0.32918288 0.6708171
350.1   0.60700389 0.0000000
378.1   0.60700389 0.3929961
1490.1  0.60700389 0.0000000
662.1   0.60700389 0.3929961
256.1   0.03112840 0.9688716
1090.1  0.50428016 0.4957198
1489.1  0.47704280 0.5229572
1260.1  0.60700389 0.0000000
895.NA  0.00000000 0.9859922
811.1   0.01400778 0.9859922
479.1   0.40466926 0.5953307
659.1   0.53929961 0.4607004
727.1   0.48326848 0.5167315
263.1   0.60700389 0.0000000
768.1   0.14241245 0.8575875
1552.1  0.37509728 0.6249027
403.1   0.60700389 0.0000000
308.1   0.60700389 0.0000000
412.1   0.60700389 0.0000000
1294.1  0.36342412 0.6357977
1503.1  0.60700389 0.0000000
1126.1  0.37042802 0.6295720
1270.1  0.32140078 0.6785992
596.1   0.60700389 0.0000000
92.1    0.60700389 0.0000000
483.1   0.60700389 0.0000000
1041.1  0.53852140 0.4614786
486.1   0.60700389 0.0000000
906.1   0.01400778 0.9859922
1207.1  0.20856031 0.7914397
1228.1  0.38754864 0.6124514
561.1   0.60700389 0.0000000
545.1   0.60700389 0.0000000
1282.1  0.60700389 0.0000000
922.1   0.17120623 0.8287938
1060.1  0.60700389 0.0000000
1563.1  0.13229572 0.8677043
1535.1  0.12140078 0.8785992
419.1   0.10894942 0.8910506
273.1   0.60700389 0.0000000
1263.1  0.60700389 0.0000000
1499.1  0.60700389 0.3929961
1248.1  0.60700389 0.0000000
269.1   0.15719844 0.8428016
1077.1  0.20700389 0.7929961
1029.1  0.29883268 0.7011673
1339.1  0.08404669 0.9159533
1574.1  0.47704280 0.5229572
1008.1  0.21478599 0.7852140
551.1   0.60700389 0.0000000
143.1   0.31595331 0.6840467
1311.1  0.47315175 0.5268482
400.1   0.60700389 0.0000000
834.1   0.53852140 0.4614786
156.1   0.60700389 0.3929961
1185.1  0.41556420 0.5844358
448.1   0.29182879 0.7081712
1102.1  0.28715953 0.7128405
377.1   0.60700389 0.0000000
1468.1  0.60700389 0.0000000
726.1   0.60700389 0.0000000
1463.1  0.02178988 0.9782101
254.1   0.04824903 0.9517510
1452.1  0.05291829 0.9470817
788.1   0.32062257 0.6793774
181.1   0.51050584 0.4894942
359.1   0.60700389 0.0000000
1003.1  0.60700389 0.0000000
1343.1  0.03346304 0.9665370
140.1   0.31984436 0.6801556
1064.1  0.46225681 0.5377432
1321.1  0.01400778 0.9859922
1140.1  0.15019455 0.8498054
429.1   0.39688716 0.6031128
903.1   0.21634241 0.7836576
1325.1  0.60700389 0.3929961
1324.1  0.60700389 0.3929961
472.1   0.47003891 0.5299611
1516.1  0.20856031 0.7914397
230.1   0.60700389 0.0000000
446.1   0.28326848 0.7167315
706.1   0.60700389 0.0000000
1277.NA 0.00000000 0.9859922
1383.1  0.60700389 0.0000000
356.1   0.60700389 0.0000000
65.1    0.60700389 0.0000000
576.1   0.60700389 0.0000000
1176.1  0.49805447 0.5019455
478.1   0.60700389 0.0000000
1399.1  0.38132296 0.6186770
515.1   0.60700389 0.0000000
867.1   0.01400778 0.9859922
536.1   0.60700389 0.0000000
946.1   0.60700389 0.0000000
1575.1  0.13385214 0.8661479
1576.1  0.36108949 0.6389105
588.1   0.04280156 0.9571984
1509.1  0.60700389 0.0000000
1426.1  0.60700389 0.0000000
790.1   0.01400778 0.9859922
1242.1  0.36342412 0.6357977
1500.1  0.47704280 0.5229572
566.1   0.60700389 0.0000000
288.1   0.60700389 0.0000000
584.1   0.60700389 0.0000000
1375.1  0.60700389 0.0000000
625.1   0.09883268 0.9011673
1280.1  0.56186770 0.4381323
1034.1  0.60700389 0.0000000
1265.NA 0.00000000 0.9859922
638.1   0.43112840 0.5688716
817.1   0.06225681 0.9377432
656.1   0.56031128 0.4396887
147.1   0.42023346 0.5797665
335.1   0.06692607 0.9330739
1481.1  0.47704280 0.5229572
503.1   0.02178988 0.9782101
1457.1  0.41400778 0.5859922
1381.1  0.53852140 0.4614786
1606.1  0.60700389 0.0000000
374.1   0.60700389 0.0000000
1058.1  0.60700389 0.0000000
943.1   0.60700389 0.0000000
1492.1  0.13463035 0.8653696
87.1    0.60700389 0.0000000
1011.1  0.60700389 0.3929961
831.1   0.24046693 0.7595331
227.1   0.60700389 0.3929961
84.1    0.60700389 0.0000000
146.1   0.32529183 0.6747082
510.1   0.60700389 0.0000000
226.1   0.60700389 0.3929961
1541.1  0.50428016 0.4957198
1460.1  0.60700389 0.3929961
343.1   0.08404669 0.9159533
241.1   0.60700389 0.0000000
634.1   0.35175097 0.6474708
1023.1  0.60700389 0.3929961
1560.1  0.29883268 0.7011673
699.1   0.37509728 0.6249027
1166.1  0.02178988 0.9782101
494.1   0.60700389 0.3929961
64.1    0.60700389 0.0000000
184.1   0.28171206 0.7182879
425.1   0.60700389 0.0000000
205.1   0.41556420 0.5844358
175.1   0.19610895 0.8038911
292.1   0.60700389 0.0000000
298.1   0.56186770 0.4381323
173.1   0.60700389 0.0000000
1290.1  0.60700389 0.0000000
430.1   0.60700389 0.0000000
666.1   0.39688716 0.6031128
894.1   0.06225681 0.9377432
1538.1  0.18210117 0.8178988
1410.NA 0.00000000 0.9859922
66.1    0.23735409 0.7626459
1215.1  0.29105058 0.7089494
1471.1  0.25136187 0.7486381
124.1   0.29649805 0.7035019
643.1   0.19844358 0.8015564
602.1   0.60700389 0.0000000
489.1   0.60700389 0.0000000
1314.1  0.27937743 0.7206226
1498.1  0.30116732 0.6988327
1599.1  0.60700389 0.0000000
89.1    0.49805447 0.5019455
520.1   0.60700389 0.0000000
183.1   0.60700389 0.0000000
251.1   0.60700389 0.0000000
1089.1  0.60700389 0.0000000
931.1   0.01400778 0.9859922
346.1   0.60700389 0.0000000
640.1   0.60700389 0.0000000
108.1   0.31828794 0.6817121
457.1   0.47003891 0.5299611
1249.1  0.60700389 0.3929961
805.1   0.03735409 0.9626459
855.1   0.47315175 0.5268482
> example$p.values$Significance_p.values
        1 2
427.1   1 1
598.1   1 0
919.1   1 1
1456.1  1 1
324.1   1 1
1439.1  1 1
1512.1  1 0
1057.1  1 1
1006.1  1 0
99.1    1 1
329.1   1 1
282.1   1 0
1096.1  1 1
612.1   1 0
1226.1  1 1
792.1   0 1
1142.1  0 1
1577.1  0 1
604.1   1 0
1234.1  1 1
1483.1  1 1
337.1   1 0
1033.1  0 1
199.1   0 1
423.1   1 0
611.1   1 1
22.1    1 0
1588.1  1 0
1373.1  1 0
537.1   1 1
760.1   1 1
945.1   1 0
777.1   1 1
293.1   1 1
1301.1  1 0
1051.1  1 1
1247.1  1 1
170.1   1 0
1135.1  1 1
645.1   1 1
1286.1  1 0
1013.1  1 0
1225.1  1 1
865.1   1 1
828.1   1 1
1233.1  1 1
37.1    1 0
745.1   1 1
1141.1  0 1
1079.1  1 1
744.1   1 1
1340.1  0 1
681.1   1 1
381.1   0 1
110.1   1 1
155.1   1 1
491.1   0 1
804.1   0 1
1025.1  1 1
630.1   1 1
1412.1  1 0
454.1   1 1
709.1   1 0
513.1   1 1
1004.1  1 0
398.1   1 1
737.1   1 1
1180.1  0 1
130.1   1 1
1346.1  1 1
521.1   1 0
1289.1  1 0
532.1   1 1
512.1   1 0
730.1   1 1
1366.1  1 1
1323.1  1 1
597.1   1 0
1188.NA 0 1
1467.1  1 0
664.1   1 0
1087.1  1 1
610.1   1 0
496.1   1 0
1153.1  1 1
309.1   1 1
1081.1  1 0
185.1   1 1
373.1   1 0
218.1   0 1
364.1   1 0
90.1    1 0
973.1   1 1
1326.1  1 1
1178.1  1 1
1205.1  1 1
688.1   1 1
619.1   1 1
1223.1  1 1
912.1   1 1
987.1   0 1
1534.1  1 1
407.1   1 1
1493.1  1 0
952.1   0 1
321.1   1 1
195.1   0 1
717.1   1 1
1385.1  1 1
897.1   1 0
1461.1  1 1
1095.1  1 1
533.1   1 1
1567.1  1 0
222.1   1 1
20.1    1 1
1067.1  1 1
154.1   1 1
665.1   1 0
1502.1  1 0
1474.1  0 1
736.1   1 1
719.1   1 1
258.1   0 1
1119.1  0 1
673.1   1 1
757.1   1 1
307.1   1 0
338.1   1 0
880.1   0 1
849.1   0 1
114.1   1 1
53.1    0 1
947.1   1 1
1367.1  1 0
1477.1  1 1
825.1   1 0
773.1   0 1
1447.1  1 1
1559.1  1 1
1001.1  1 0
882.1   1 1
350.1   1 0
378.1   1 1
1490.1  1 0
662.1   1 1
256.1   0 1
1090.1  1 1
1489.1  1 1
1260.1  1 0
895.NA  0 1
811.1   0 1
479.1   1 1
659.1   1 1
727.1   1 1
263.1   1 0
768.1   0 1
1552.1  1 1
403.1   1 0
308.1   1 0
412.1   1 0
1294.1  1 1
1503.1  1 0
1126.1  1 1
1270.1  1 1
596.1   1 0
92.1    1 0
483.1   1 0
1041.1  1 1
486.1   1 0
906.1   0 1
1207.1  1 1
1228.1  1 1
561.1   1 0
545.1   1 0
1282.1  1 0
922.1   0 1
1060.1  1 0
1563.1  0 1
1535.1  0 1
419.1   0 1
273.1   1 0
1263.1  1 0
1499.1  1 1
1248.1  1 0
269.1   0 1
1077.1  1 1
1029.1  1 1
1339.1  0 1
1574.1  1 1
1008.1  1 1
551.1   1 0
143.1   1 1
1311.1  1 1
400.1   1 0
834.1   1 1
156.1   1 1
1185.1  1 1
448.1   1 1
1102.1  1 1
377.1   1 0
1468.1  1 0
726.1   1 0
1463.1  0 1
254.1   0 1
1452.1  0 1
788.1   1 1
181.1   1 1
359.1   1 0
1003.1  1 0
1343.1  0 1
140.1   1 1
1064.1  1 1
1321.1  0 1
1140.1  0 1
429.1   1 1
903.1   1 1
1325.1  1 1
1324.1  1 1
472.1   1 1
1516.1  1 1
230.1   1 0
446.1   1 1
706.1   1 0
1277.NA 0 1
1383.1  1 0
356.1   1 0
65.1    1 0
576.1   1 0
1176.1  1 1
478.1   1 0
1399.1  1 1
515.1   1 0
867.1   0 1
536.1   1 0
946.1   1 0
1575.1  0 1
1576.1  1 1
588.1   0 1
1509.1  1 0
1426.1  1 0
790.1   0 1
1242.1  1 1
1500.1  1 1
566.1   1 0
288.1   1 0
584.1   1 0
1375.1  1 0
625.1   0 1
1280.1  1 1
1034.1  1 0
1265.NA 0 1
638.1   1 1
817.1   0 1
656.1   1 1
147.1   1 1
335.1   0 1
1481.1  1 1
503.1   0 1
1457.1  1 1
1381.1  1 1
1606.1  1 0
374.1   1 0
1058.1  1 0
943.1   1 0
1492.1  0 1
87.1    1 0
1011.1  1 1
831.1   1 1
227.1   1 1
84.1    1 0
146.1   1 1
510.1   1 0
226.1   1 1
1541.1  1 1
1460.1  1 1
343.1   0 1
241.1   1 0
634.1   1 1
1023.1  1 1
1560.1  1 1
699.1   1 1
1166.1  0 1
494.1   1 1
64.1    1 0
184.1   1 1
425.1   1 0
205.1   1 1
175.1   0 1
292.1   1 0
298.1   1 1
173.1   1 0
1290.1  1 0
430.1   1 0
666.1   1 1
894.1   0 1
1538.1  0 1
1410.NA 0 1
66.1    1 1
1215.1  1 1
1471.1  1 1
124.1   1 1
643.1   0 1
602.1   1 0
489.1   1 0
1314.1  1 1
1498.1  1 1
1599.1  1 0
89.1    1 1
520.1   1 0
183.1   1 0
251.1   1 0
1089.1  1 0
931.1   0 1
346.1   1 0
640.1   1 0
108.1   1 1
457.1   1 1
1249.1  1 1
805.1   0 1
855.1   1 1
> 
> 
> 
> 
> cleanEx()

detaching 'package:e1071', 'package:randomForest'

> nameEx("ConformalRegression")
> ### * ConformalRegression
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ConformalRegression
> ### Title: Conformal Prediction for Regression
> ### Aliases: ConformalRegression
> 
> ### ** Examples
> 
> #############################################
> ### Example
> #############################################
> 
> # Optional for parallel training
> #library(doMC)
> #registerDoMC(cores=4)
> 
> data(LogS)
> 
> algorithm <- "svmRadial"
> tune.grid <- expand.grid(.sigma = expGrid(power.from=-10, power.to=-6, power.by=1, base=2), 
+                          .C = expGrid(power.from=4, power.to=10, power.by=2, base=2))
> trControl <- trainControl(method = "cv",  number=5,savePredictions=TRUE)
> set.seed(3)
> model <- train(LogSDescsTrain, LogSTrain, algorithm, 
+                tuneGrid=tune.grid, 
+                trControl=trControl)
Loading required package: kernlab
Warning: package 'kernlab' was built under R version 3.0.2
> 
> 
> # Train an error model
> error_model <- ErrorModel(PointPredictionModel=model,x.train=LogSDescsTrain,
+                           savePredictions=TRUE,algorithm=algorithm,
+                           trControl=trControl, tune.grid=tune.grid)
> 
> # Instantiate the class and get the confidence intervals
> example <- ConformalRegression$new()
Conformal Prediction Class for Regression Instantiated
> example$CalculateAlphas(model=model,error_model=error_model,ConformityMeasure=StandardMeasure)
[1] "Calculating alphas.."

> example$GetConfidenceIntervals(new.data=LogSDescsTest)
[1] "Predicting (i) the value, and (ii) the error for the new data.."

> example$CorrelationPlot(obs=LogSTest)
Error in is.na(intervals) : 'intervals' is missing
Calls: <Anonymous>
Execution halted
